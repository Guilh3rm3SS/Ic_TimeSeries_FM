# -*- coding: utf-8 -*-
"""mercedes darts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bAi_faXp69Q21kHNyRl3uQubGuIZuolA
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from darts import TimeSeries
from darts.dataprocessing.transformers import Scaler
from darts.utils.utils import generate_index

url = "https://drive.google.com/uc?id=1qAi5oqUUp-i34MoW5fg_G1o6iFIKlidJ"
full_df = pd.read_csv(url, index_col=0, parse_dates=True)
print(full_df.head())
df = full_df[['dt', 'Mercedes_level']]

df['dt'] = pd.to_datetime(df['dt'], format='%d/%m/%Y %H:%M')

# train_split = int(len(df) * 0.6)
# train_df = df.iloc[:train_split]
# test_df = df.iloc[train_split:]

series = TimeSeries.from_dataframe(df, time_col='dt')
# series.plot()
# # O dataframe de validação é obtido a partir do de treino
# validation_split = int(len(train_df) * 0.8)
# train_df = train_df.iloc[:validation_split]
# val_df = train_df.iloc[validation_split:]

"""#Backtests"""

# def train_model(
#     train,
#     val,
#     context_len=30,
#     horizon_len=1,
#     model='LSTM',
#     epochs=20,
#     learning_rate=1e-3,
#     dropout=0.1
# ):

#   model = RNNModel(
#       model=model,
#       training_length=int((context_len+horizon_len)*1.5),
#       input_chunk_length=context_len,   # janela de entrada
#       output_chunk_length=horizon_len,  # previsão à frente
#       n_epochs=epochs,
#       random_state=42,
#       optimizer_kwargs={"lr": learning_rate},
#       # pl_trainer_kwargs={"enable_checkpointing": True, "early_stopping": True}
#       dropout=0.2

#   )

#   model.fit(
#       series=train,
#       val_series=val,
#       verbose=True
# )

#   return model

# def train_optimized_model(
#     train,
#     context_len=30,
#     horizon_len=1,
#     model='LSTM',
#     optimized_params=None,
#     epochs=20,
#     learning_rate=1e-3,


# ):
#     # Otimização com Optuna
#     study = optuna.create_study(direction="minimize")
#     study.optimize(objective, n_trials=n_trials)

#     print("Melhores parâmetros encontrados:")
#     print(study.best_trial.params)

#     # Treinamento final com melhores parâmetros
#     best_params = study.best_trial.params
#     best_model = RNNModel(
#         model=model_type,
#         input_chunk_length=context_len,
#         output_chunk_length=horizon_len,
#         hidden_dim=best_params["hidden_dim"],
#         n_rnn_layers=best_params["n_rnn_layers"],
#         dropout=best_params["dropout"],
#         batch_size=32,
#         n_epochs=epochs,
#         random_state=42,
#         optimizer_cls=AdamW,
#         optimizer_kwargs={"lr": 1e-3, "weight_decay": 1e-4},
#         lr_scheduler_cls=lambda opt: SequentialLR(
#             opt,
#             schedulers=[
#                 LinearLR(opt, start_factor=0.1, total_iters=3),
#                 CosineAnnealingLR(opt, T_max=epochs - 3)
#             ],
#             milestones=[3]
#         ),
#         pl_trainer_kwargs={
#             "accelerator": "auto",
#             "callbacks": [EarlyStopping(monitor="val_loss", patience=3)]
#         }
#     )

#     best_model.fit(series=train, val_series=val, verbose=True)
#     return best_model, study

import torch
from torch.optim import AdamW
from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
from darts.models import RNNModel

class CustomRNNModel(RNNModel):
    def configure_optimizers(self):
        optimizer = AdamW(self.parameters(), lr=1e-3, weight_decay=1e-4)

        # Warmup de 3 epochs
        warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=3)
        # Cosine decay após warmup
        cosine_scheduler = CosineAnnealingLR(optimizer, T_max=self.n_epochs - 3)

        scheduler = SequentialLR(
            optimizer,
            schedulers=[warmup_scheduler, cosine_scheduler],
            milestones=[3]
        )

        return [optimizer], [scheduler]

def make_scheduler_factory(epochs):
    def scheduler_factory(optimizer):
        return SequentialLR(
            optimizer,
            schedulers=[
                LinearLR(optimizer, start_factor=0.1, total_iters=3),
                CosineAnnealingLR(optimizer, T_max=epochs - 3)
            ],
            milestones=[3]
        )
    return scheduler_factory

import optuna
from pytorch_lightning.callbacks import EarlyStopping
from darts.metrics import smape

def train_model_hyperparam_search(
    train,
    val,
    context_len=30,
    horizon_len=1,
    model_type='LSTM',
    epochs=40,
    n_trials=20
):

    def objective(trial):
        hidden_dim = trial.suggest_int("hidden_dim", 16, 128, log=True)
        n_rnn_layers = trial.suggest_int("n_rnn_layers", 1, 4)
        dropout = trial.suggest_float("dropout", 0.0, 0.5)

        early_stopper = EarlyStopping(monitor="val_loss", patience=5, verbose=False)

        model = CustomRNNModel(
            model=model_type,
            training_length=int((context_len + horizon_len) * 1.5),
            input_chunk_length=context_len,
            output_chunk_length=horizon_len,
            hidden_dim=hidden_dim,
            n_rnn_layers=n_rnn_layers,
            dropout=dropout,
            batch_size=32,
            n_epochs=epochs,
            random_state=42,
            pl_trainer_kwargs={"callbacks": [early_stopper]},
        )

        model.fit(train, val_series=val, verbose=False)
        preds = model.predict(n=horizon_len, series=train)
        return smape(val, preds)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)

    print("Melhores parâmetros:", study.best_trial.params)


    # Otimização com Optuna
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)

    print("Melhores parâmetros encontrados:")
    print(study.best_trial.params)

    # Treinamento com parâmetros otimizados usando apenas o treino
    best_params = study.best_trial.params
    best_model = RNNModel(
      model=model_type,
      training_length=int((context_len + horizon_len) * 1.5),
      input_chunk_length=context_len,
      output_chunk_length=horizon_len,
      hidden_dim=best_params["hidden_dim"],
      n_rnn_layers=best_params["n_rnn_layers"],
      dropout=best_params["dropout"],
      batch_size=32,
      n_epochs=epochs,
      random_state=42,
      optimizer_cls=AdamW,
      optimizer_kwargs={"lr": 1e-3, "weight_decay": 1e-4},
      lr_scheduler_cls=make_scheduler_factory(epochs),
      pl_trainer_kwargs={
            "accelerator": "auto",
            "callbacks": [EarlyStopping(monitor="val_loss", patience=5)]
        }
    )   
    
    # full_train = train.concatenate(val)
    best_model.fit(series=train, val_series=val, verbose=True)
    # return best_model, study
    return best_model

def backtest_model(
    test_series,
    horizon_len,
    model
):
  forecast_series = model.historical_forecasts(
    test_series,
    forecast_horizon=horizon_len,
    retrain=False,
    verbose=True,
    last_points_only=True,
    stride=1
  )

  return forecast_series

from darts.models import RNNModel
from sklearn.preprocessing import MaxAbsScaler

context_lengths = [7, 32, 160, 352]
horizon_lengths = [1, 7, 15, 30]

# context_lengths = [32]
# horizon_lengths = [1, 7]

train, test = series.split_after(0.60)
train, val = train.split_after(0.70)


base_result = test.to_dataframe()
base_result.rename(columns={"Mercedes_level": "observed"}, inplace=True)


scaler = Scaler(MaxAbsScaler())
train = scaler.fit_transform(train)
val = scaler.transform(val)
test = scaler.transform(test)

# model = train_model(train, val, context_len=30, horizon_len=7, model='LSTM', epochs=20, learning_rate=1e-3, dropout=0.1)
# forecast = backtest_model(test, 7, model)

# plt.figure(figsize=(8, 5))
# test.plot(label="actual")
# forecast.plot(label="backtest")

for context_len in context_lengths:
  for horizon_len in horizon_lengths:
    if ((1.5*(context_len + horizon_len)) + 1) > len(val):
      continue

    print(f"Context length: {context_len}, Horizon length: {horizon_len}")
    column_name = f"mercedes_level_{context_len}_{horizon_len}"
    model = train_model_hyperparam_search(train, val, context_len=context_len, horizon_len=horizon_len, model_type='LSTM')
    forecast = backtest_model(test, horizon_len, model)
    forecast = scaler.inverse_transform(forecast)
    forecast_df = forecast.to_dataframe()
    forecast_df.rename(columns={"Mercedes_level": column_name}, inplace=True)
    base_result = base_result.merge(forecast_df, on="dt", how="left")

print(base_result)
path = 'Mercedes_LSTM_opt_40_earlys_no_cov.csv'

# Salva como CSV
base_result.to_csv(path, index=True)

