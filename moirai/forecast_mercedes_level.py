# -*- coding: utf-8 -*-
"""Moirai Moe - Forecast - Mercedes_Level

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ftO7RiEftVjCySryTODanduaDEN3g8KU
"""

import pandas as pd
import numpy as np
import torch
from einops import rearrange
from gluonts.dataset.multivariate_grouper import MultivariateGrouper
from gluonts.dataset.pandas import PandasDataset
from gluonts.dataset.split import split

from uni2ts.eval_util.plot import plot_single, plot_next_multi
from uni2ts.model.moirai import MoiraiForecast, MoiraiModule
from uni2ts.model.moirai_moe import MoiraiMoEForecast, MoiraiMoEModule

MODEL = "moirai-moe"  # model name: choose from {'moirai', 'moirai-moe'}

# Cria dataframe, mudar para o nome do arquivo importado
url = "https://drive.google.com/uc?id=1qAi5oqUUp-i34MoW5fg_G1o6iFIKlidJ"
df = pd.read_csv(url, index_col=0, parse_dates=True)

# Converte a coluna de data do df para datetime
df['dt'] = pd.to_datetime(df['dt'], format='%d/%m/%Y %H:%M')
df = df.drop(df.columns[20], axis=1)

# Divide a série temporal
split_index = int(len(df) * 0.7)
df = df.iloc[split_index:]
# df.set_index('dt', inplace=True)


# Define as colunas do df que serão consideradas como covariáveis, caso não haja, pode ficar vazio
covariables = [
    "Bonete_level", "Manuel_Díaz_level", "Cuñapirú_level", "Mazangano_level", "Coelho_level",
    "Paso_de_las_Toscas_level", "Aguiar_level", "Laguna_I_level", "Laguna_II_level", "Pereira_level",
    "San_Gregorio_level", "Paso_de_los_Toros_level", "Salsipuedes_level", "Sarandi_del_Yi_level",
    "Durazno_level", "Polanco_level", "Lugo_level",
    "Bonete_precipitation", "Manuel_Diaz_precipitation", "Cuñapirú_precipitation", "Mazagano_precipitation",
    "Coelho_precipitation", "Paso_de_las_Toscas_precipitation", "Aguiar_precipitation", "Laguna_I_precipitation",
    "Laguna_II_precipitation", "Pereira_precipitation", "San_Gregorio_precipitation",
    "Paso_de_los_toros_precipitation", "Salsipuedes_precipitation", "Sarandi_del_Yi_precipitation",
    "Polanco_precipitation", "Durazno_precipitation", "Paso_de_Lugo_precipitation",
    "Mercedes_precipitation"
]

only_levels = [
    "Bonete_level", "Manuel_Díaz_level", "Cuñapirú_level", "Mazangano_level", "Coelho_level",
    "Paso_de_las_Toscas_level", "Aguiar_level", "Laguna_I_level", "Laguna_II_level", "Pereira_level",
    "San_Gregorio_level", "Paso_de_los_Toros_level", "Salsipuedes_level", "Sarandi_del_Yi_level",
    "Durazno_level", "Polanco_level", "Lugo_level",
]

mercedes_precipitation = ["Mercedes_precipitation"]


# Define a coluna target, que será a que queremos prever
target = 'Mercedes_level'

"""Definição das Funções para avaliar o erro do modelo"""

from scipy.stats import linregress

def mse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.mean((y_pred - y_true) ** 2)

def mae(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.mean(np.abs(y_pred - y_true))

def r_squared(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot == 0:
        return np.nan
    return 1 - (ss_res / ss_tot)

def rmse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.sqrt(np.mean((y_pred - y_true) ** 2))

def nse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    numerator = np.sum((y_true - y_pred) ** 2)
    denominator = np.sum((y_true - np.mean(y_true)) ** 2)
    if denominator == 0:
        return np.nan
    return 1 - (numerator / denominator)

def pbias(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    total_error = np.sum(y_pred - y_true)
    total_true = np.sum(y_true)
    if total_true == 0:
        return np.nan
    return 100 * (total_error / total_true)

def ve(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    numerator = np.sum(np.abs(y_pred - y_true))
    denominator = np.sum(y_true)
    if denominator == 0:
        return np.nan
    return 1 - (numerator / denominator)


def p_value(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    slope, intercept, r_value, p_value, std_err = linregress(y_true, y_pred)
    return p_value



# Convert into GluonTS dataset
def create_dataset(
    df,
    SIZE = "base",  # model size: choose from {'small', 'base', 'large'}
    PDT = 20,  # prediction length: any positive integer
    CTX = 200,  # context length: any positive integer
    PSZ = "auto",  # patch size: choose from {"auto", 8, 16, 32, 64, 128}
    BSZ = 32,  # batch size: any positive integer
    TEST = 658,  # test set length: any positive integer
    COV = None, # list of covariates
):
  ds = PandasDataset(
  df,
  target=target,
  timestamp="dt",
  freq='D',
  past_feat_dynamic_real=COV,
  )

  # Split into train/test set
  _, test_template = split(
      ds, offset=-TEST
  )  # assign last TEST time steps as test set

  # Construct rolling window evaluation
  test_data = test_template.generate_instances(
      prediction_length=PDT,  # number of time steps for each prediction
      windows=(TEST - PDT) + 1,  # number of windows in rolling window evaluation
      distance=1,  # number of time steps between each window - distance=PDT for non-overlapping windows
  )


  return ds, test_data

from datetime import timedelta
import pandas as pd
import numpy as np

def create_results_dataframe(labels, forecasts, name="forecast"):
    last_dates = []
    last_forecast_values = []
    last_label_values = []
    for forecast, label in zip(forecasts, labels):
        # Converta o start_date de Period para Timestamp
        start = forecast.start_date.to_timestamp()
        prediction_length = forecast.samples.shape[1]

        # Último timestamp
        last_timestamp = start + timedelta(days=prediction_length - 1)

        last_label_value = label['target'][-1]
        # Valor previsto (usando a média entre amostras, se houver mais de uma)
        # Ensure the forecast samples are numpy arrays to use .mean()
        last_forecast_value = np.array(forecast.samples)[:, -1].mean()

        last_dates.append(last_timestamp)
        last_forecast_values.append(last_forecast_value)
        last_label_values.append(last_label_value)

    return_df = pd.DataFrame({"dt": last_dates, name: last_forecast_values})

    return return_df # Explicitly return the DataFrame

# Prepare model
def make_prediction(
    df,
    SIZE = "base",  # model size: choose from {'small', 'base', 'large'}
    PDT = 20,  # prediction length: any positive integer
    CTX = 200,  # context length: any positive integer
    PSZ = "auto",  # patch size: choose from {"auto", 8, 16, 32, 64, 128}
    BSZ = 32,  # batch size: any positive integer
    TEST = 658,  # test set length: any positive integer
    SAMPLES = 1, # number of samples for the prediction
    COV = None, # list of covariates
    NAME = "forecast"
):

  ds, test_data = create_dataset(df, CTX=CTX, PDT=PDT, TEST=TEST, COV=COV)
  if MODEL == "moirai":
      model = MoiraiForecast(
          module=MoiraiModule.from_pretrained(f"Salesforce/moirai-1.1-R-{SIZE}"),
          prediction_length=PDT,
          context_length=CTX,
          patch_size=PSZ,
          num_samples=SAMPLES,
          target_dim=1,
          feat_dynamic_real_dim=ds.num_feat_dynamic_real,
          past_feat_dynamic_real_dim=ds.num_past_feat_dynamic_real
      )
  elif MODEL == "moirai-moe":
      model = MoiraiMoEForecast(
          module=MoiraiMoEModule.from_pretrained(f"Salesforce/moirai-moe-1.0-R-{SIZE}"),
          prediction_length=PDT,
          context_length=CTX,
          patch_size=16,
          num_samples=SAMPLES,
          target_dim=1,
          feat_dynamic_real_dim=ds.num_feat_dynamic_real,
          past_feat_dynamic_real_dim=ds.num_past_feat_dynamic_real
      )

  predictor = model.create_predictor(batch_size=BSZ)

  input_set = test_data
  # input_set.input[:]['target'] = input_set.input[CTX:]

  forecasts = predictor.predict(input_set.input)

  # print(len(list(iter(test_data.input))[-1]['target']))
  # print(list(iter(test_data.input))[:10])
  # print(list(iter(test_data.label))[:10])

  input_it = iter(test_data.input)
  label_it = iter(test_data.label)
  forecast_it = iter(forecasts)

  dataframe = create_results_dataframe(test_data.label, forecasts, name=NAME)

  return dataframe

hyperparams = [(50, 7, 1), (50, 7, 7), (50, 7, 30), (50, 160, 1), (50, 160, 7), (50, 160, 30), (50, 365, 1), (50, 365, 7), (50, 365, 30), (50, 500, 1), (50, 500, 7), (50, 500, 30)]

# colunas = ["Tag", "MAE", "MSE", "R2", "RMSE", "PBias", "VE"]

# DataFrame base com as datas e valores observados
df_base = df[["dt", target]].copy()
df_base['dt'] = pd.to_datetime(df_base['dt']).dt.floor('D')
df_base.rename(columns={target: 'observed'}, inplace=True)
df_base = df_base.reset_index(names="index_dt")
df_base = df_base.drop(columns="index_dt")
print(df_base.head(3))

for batch_size, context_len, horizon_len in hyperparams:
  # Previsão sem covariáveis
  simple_name = (f"Mercedes_Level_no_Covariables_{context_len}_{horizon_len}")
  # simple_data = simple_forecast(df, target, batch_size, context_len, horizon_len, simple_name)
  simple_data = make_prediction(df=df, CTX=context_len, PDT=horizon_len, TEST=len(df)-context_len, COV=None, NAME=simple_name)
  print(simple_data.head(3))
  df_base = df_base.merge(simple_data, on="dt", how="left")
  # Previsão com a covariavel de chuva no mercedes
  one_covar_name = (f"Mercedes_Level_+_Mercedes_Rain_{context_len}_{horizon_len}")
  # one_covar_data = covariable_forecast(df, mercedes_precipitation, target, batch_size, context_len, horizon_len, one_covar_name)
  one_covar_data = make_prediction(df, CTX=context_len, PDT=horizon_len, TEST=len(df)-context_len, COV=mercedes_precipitation, NAME=one_covar_name)
  df_base = df_base.merge(one_covar_data, on="dt", how="left")
  # Previsão com a covariavel de chuva no mercedes com lag de -4
  # lag_covar_name = (f"Mercedes_Level_+_Mercedes_Rain_Lag_{context_len}_{horizon_len}")
  # lag_covar_data = covariable_forecast(df_lagged, mercedes_precipitation_lagged, target, batch_size, context_len, horizon_len, lag_covar_name)
  # df_base = df_base.merge(lag_covar_data, on="dt", how="left")
  # Previsão com todas as covariáveis de nível de chuva
  all_level_name = (f"Mercedes_Level_+_All_Levels_{context_len}_{horizon_len}")
  # all_level_data = covariable_forecast(df, only_levels, target, batch_size, context_len, horizon_len, all_level_name)
  all_level_data = make_prediction(df, CTX=context_len, PDT=horizon_len, TEST=len(df)-context_len, COV=only_levels, NAME=all_level_name)
  df_base = df_base.merge(all_level_data, on="dt", how="left")
  # Previsão com todas as covariáveis do data frame
  all_covar_name = (f"Mercedes_Level_+_All_Levels_+_All_Rain_{context_len}_{horizon_len}")
  # all_covar_data = covariable_forecast(df, covariables, target, batch_size, context_len, horizon_len, all_covar_name)
  all_covar_data = make_prediction(df, CTX=context_len, PDT=horizon_len, TEST=len(df)-context_len, COV=covariables, NAME=all_covar_name)

  df_base = df_base.merge(all_covar_data, on="dt", how="left")
  print(f"{context_len}/{horizon_len}: concluido")

# Caminho para salvar no Google Drive (ajuste a pasta se quiser)
output_path = "Resultados_Mercedes_forecast_base.csv"

# Salvar CSV
df_base.to_csv(output_path, index=False)

print(f"CSV salvo em: {output_path}")

