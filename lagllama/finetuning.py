# -*- coding: utf-8 -*-
"""Mercedes Forecast LagLlama

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KHBemGzBA2h1OMJDTNwO6ZSuMHQQ9ths
"""

# # Commented out IPython magic to ensure Python compatibility.
# git clone -b update-gluonts https://github.com/time-series-foundation-models/lag-llama/

# # %cd /content/lag-llama

# pip install -r requirements.txt

# pip install -U torch torchvision

# # %cd /content/lag-llama

# huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama

from collections import defaultdict
from itertools import islice

from matplotlib import pyplot as plt
import matplotlib.dates as mdates

import torch
from gluonts.evaluation import make_evaluation_predictions, Evaluator
from gluonts.dataset.repository.datasets import get_dataset

from gluonts.dataset.pandas import PandasDataset
import pandas as pd
import numpy as np

from lag_llama.gluon.estimator import LagLlamaEstimator

import sys
from types import ModuleType

# Create dummy module hierarchy
def create_dummy_module(module_path):
    """
    Create a dummy module hierarchy for the given path.
    Returns the leaf module.
    """
    parts = module_path.split('.')
    current = ''
    parent = None

    for part in parts:
        current = current + '.' + part if current else part
        if current not in sys.modules:
            module = ModuleType(current)
            sys.modules[current] = module
            if parent:
                setattr(sys.modules[parent], part, module)
        parent = current

    return sys.modules[module_path]

# Create the dummy gluonts module hierarchy
gluonts_module = create_dummy_module('gluonts.torch.modules.loss')

# Create dummy classes for the specific loss functions
class DistributionLoss:
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return 0.0

    def __getattr__(self, name):
        return lambda *args, **kwargs: None

class NegativeLogLikelihood:
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return 0.0

    def __getattr__(self, name):
        return lambda *args, **kwargs: None

# Add the specific classes to the module
gluonts_module.DistributionLoss = DistributionLoss
gluonts_module.NegativeLogLikelihood = NegativeLogLikelihood

# Carrega arquivo csv
url = "https://drive.google.com/uc?id=1qAi5oqUUp-i34MoW5fg_G1o6iFIKlidJ"
df = pd.read_csv(url, index_col=0, parse_dates=True)


# Apaga a coluna duplicada do dt
df = df.drop(columns=df.columns[21])

# Converte a coluna de data do df para datetime
df['dt'] = pd.to_datetime(df['dt'], format='%d/%m/%Y %H:%M')

# Divide a série temporal
split_index = int(len(df) * 0.6)
df_train = df.iloc[:split_index]

split_validation = int(len(df_train) * 0.8)
df_val = df_train.iloc[split_validation:]
df_train = df_train.iloc[:split_validation]

df = df.iloc[split_index:]

# Previsão com lag de -4

df_lagged = df.copy()
df_lagged['Mercedes_precipitation_lagged'] = df_lagged['Mercedes_precipitation'].shift(4)
df_lagged = df_lagged.iloc[4:]

# Define as colunas do df que serão consideradas como covariáveis, caso não haja, pode ficar vazio
covariables = [
    "Bonete_level", "Manuel_Díaz_level", "Cuñapirú_level", "Mazangano_level", "Coelho_level",
    "Paso_de_las_Toscas_level", "Aguiar_level", "Laguna_I_level", "Laguna_II_level", "Pereira_level",
    "San_Gregorio_level", "Paso_de_los_Toros_level", "Salsipuedes_level", "Sarandi_del_Yi_level",
    "Durazno_level", "Polanco_level", "Lugo_level",
    "Bonete_precipitation", "Manuel_Diaz_precipitation", "Cuñapirú_precipitation", "Mazagano_precipitation",
    "Coelho_precipitation", "Paso_de_las_Toscas_precipitation", "Aguiar_precipitation", "Laguna_I_precipitation",
    "Laguna_II_precipitation", "Pereira_precipitation", "San_Gregorio_precipitation",
    "Paso_de_los_toros_precipitation", "Salsipuedes_precipitation", "Sarandi_del_Yi_precipitation",
    "Polanco_precipitation", "Durazno_precipitation", "Paso_de_Lugo_precipitation",
    "Mercedes_precipitation"
]

only_levels = [
    "Bonete_level", "Manuel_Díaz_level", "Cuñapirú_level", "Mazangano_level", "Coelho_level",
    "Paso_de_las_Toscas_level", "Aguiar_level", "Laguna_I_level", "Laguna_II_level", "Pereira_level",
    "San_Gregorio_level", "Paso_de_los_Toros_level", "Salsipuedes_level", "Sarandi_del_Yi_level",
    "Durazno_level", "Polanco_level", "Lugo_level",
]

mercedes_precipitation = ["Mercedes_precipitation"]

mercedes_precipitation_lagged = ["Mercedes_precipitation_lagged"]

# Define a coluna target, que será a que queremos prever
target = 'Mercedes_level'

# Função que separa os dados em batches
def get_batched_data_fn(
    df, covariables, target,
    batch_size,
    context_len,
    horizon_len,
):

    examples = defaultdict(list)
    num_examples = 0
    # Itera pelos timepoints definindo os pontos de contexto e os de horizonte (Os que devem ser previstos)
    for start in range(0, len(df) - (context_len + horizon_len), horizon_len):
        num_examples += 1

        # Pega os pontos de contexto (Input) do Target
        examples["inputs"].append(df[target].iloc[start:start + context_len].values.tolist())
        # Pega os pontos de horizonte (Output) do Target
        examples["outputs"].append(df[target].iloc[start + context_len:start + context_len + horizon_len].tolist())
        # Pega os datetimes do Target
        examples["dt"].append(df["dt"].iloc[start + context_len:start + context_len + horizon_len].tolist())

        # Pega os pontos para cada uma das covariáveis (O número de pontos das covariáveis é igual a soma dos pontos do input e output do exemplo)
        for covar in covariables:
            examples[covar].append(df[covar].iloc[start:start + context_len + horizon_len].values.tolist())

    # Função que separa os exemplos em batches de acordo com o batch size
    def data_fn():
        for i in range(1 + (num_examples - 1) // batch_size):
            yield {k: v[(i * batch_size) : ((i + 1) * batch_size)] for k, v in examples.items()}

    return data_fn

"""Definição das Funções para avaliar o erro do modelo"""

def mse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.mean((y_pred - y_true) ** 2)

def mae(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.mean(np.abs(y_pred - y_true))

def r_squared(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    if ss_tot == 0:
        return np.nan
    return 1 - (ss_res / ss_tot)

def rmse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    return np.sqrt(np.mean((y_pred - y_true) ** 2))

def nse(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    numerator = np.sum((y_true - y_pred) ** 2)
    denominator = np.sum((y_true - np.mean(y_true)) ** 2)
    if denominator == 0:
        return np.nan
    return 1 - (numerator / denominator)

def pbias(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    total_error = np.sum(y_pred - y_true)
    total_true = np.sum(y_true)
    if total_true == 0:
        return np.nan
    return 100 * (total_error / total_true)

def ve(y_pred, y_true):
    y_pred = np.array(y_pred)
    y_true = np.array(y_true)
    numerator = np.sum(np.abs(y_pred - y_true))
    denominator = np.sum(y_true)
    if denominator == 0:
        return np.nan
    return 1 - (numerator / denominator)

from gluonts.dataset.common import ListDataset

def create_sliding_window_dataset(series, start_time, freq, context_length, prediction_length, stride=1):
    dataset = []

    total_length = len(series)
    end_idx = total_length - context_length - prediction_length + 1

    for i in range(0, end_idx, stride):
        window = {
            "start": start_time + pd.Timedelta(i, unit=freq),
            "target": series[i: i + context_length + prediction_length],
        }
        dataset.append(window)

    return ListDataset(dataset, freq=freq)

"""Fazer teste de previsão sem covariáveis"""

def get_lag_llama_predictions(dataset, prediction_length, device, context_length=32, use_rope_scaling=False, num_samples=100, forecast_name="forecast"):
    ckpt = torch.load("lag-llama.ckpt", map_location=device, weights_only=False) # Uses GPU since in this Colab we use a GPU.
    estimator_args = ckpt["hyper_parameters"]["model_kwargs"]

    rope_scaling_arguments = {
        "type": "linear",
        "factor": max(1.0, (context_length + prediction_length) / estimator_args["context_length"]),
    }

    estimator = LagLlamaEstimator(
        ckpt_path="lag-llama.ckpt",
        prediction_length=prediction_length,
        context_length=context_length, # Lag-Llama was trained with a context length of 32, but can work with any context length

        # estimator args
        input_size=estimator_args["input_size"],
        n_layer=estimator_args["n_layer"],
        n_embd_per_head=estimator_args["n_embd_per_head"],
        n_head=estimator_args["n_head"],
        scaling=estimator_args["scaling"],
        time_feat=estimator_args["time_feat"],
        rope_scaling=rope_scaling_arguments if use_rope_scaling else None,

        batch_size=1,
        num_parallel_samples=100,
        device=device,
    )

    lightning_module = estimator.create_lightning_module()
    transformation = estimator.create_transformation()
    predictor = estimator.create_predictor(transformation, lightning_module)

    forecast_it, ts_it = make_evaluation_predictions(
        dataset=dataset,
        predictor=predictor,
        num_samples=num_samples
    )
    forecasts = list(forecast_it)
    tss = list(ts_it)


    # return forecasts, tss

    # Monta o DataFrame de resultados
    rows = []
    for forecast, ts in zip(forecasts, tss):
        if isinstance(ts.index, pd.PeriodIndex):
            ts.index = ts.index.to_timestamp()

        # Último timestamp previsto
        last_dt = ts.index[-1] + (prediction_length * ts.index.freq)

        # Último valor da média da previsão
        last_value = forecast.mean[-1]

        df_part = pd.DataFrame({
            "dt": [last_dt],
            forecast_name: [last_value]
        })
        rows.append(df_part)

    result_df = pd.concat(rows).reset_index(drop=True)
    result_df
    return result_df

def finetune(context_length, prediction_length, num_samples=1, test=None, train=None, forecast_name="forecast", epochs=2, device=torch.device("cpu")):
    ckpt = torch.load("lag-llama.ckpt", map_location=device, weights_only=False)
    estimator_args = ckpt["hyper_parameters"]["model_kwargs"]
    
    # early_stopping = EarlyStopping(
    # monitor="val_loss",     # ou outro nome de métrica de validação
    # patience=5,
    # mode="min"
    # )
    
    estimator = LagLlamaEstimator(
            ckpt_path="lag-llama.ckpt",
            prediction_length=prediction_length,
            context_length=context_length,

            # distr_output="neg_bin",
            # scaling="mean",
            nonnegative_pred_samples=True,
            aug_prob=0,
            lr=5e-4,

            # estimator args
            input_size=estimator_args["input_size"],
            n_layer=estimator_args["n_layer"],
            n_embd_per_head=estimator_args["n_embd_per_head"],
            n_head=estimator_args["n_head"],
            time_feat=estimator_args["time_feat"],

            # rope_scaling={
            #     "type": "linear",
            #     "factor": max(1.0, (context_length + prediction_length) / estimator_args["context_length"]),
            # },

            batch_size=64,
            num_parallel_samples=num_samples,
            trainer_kwargs = {"max_epochs": epochs,}, # <- lightning trainer arguments
        )
    
    predictor = estimator.train(train, cache_data=True, shuffle_buffer_length=1000)
    
    forecast_it, ts_it = make_evaluation_predictions(
    dataset=test,
    predictor=predictor,
    num_samples=num_samples

    )
    print("now forecasting\n\n")
    
    forecasts = list(forecast_it)
    tss = list(ts_it)
    #print(forecasts)


    # return forecasts, tss

    # Monta o DataFrame de resultados
    rows = []
    for forecast, ts in zip(forecasts, tss):
        if isinstance(ts.index, pd.PeriodIndex):
            ts.index = ts.index.to_timestamp()

        # Último timestamp previsto
        last_dt = ts.index[-1] + (prediction_length * ts.index.freq)

        # Último valor da média da previsão
        last_value = forecast.mean[-1]
        # print(f"last value: {last_value} at {last_dt}")

        df_part = pd.DataFrame({
            "dt": [last_dt],
            forecast_name: [last_value]
        })
        rows.append(df_part)

    result_df = pd.concat(rows).reset_index(drop=True)
    print(result_df)
    return result_df


def convert_df_to_long(df):
    """Converte o DataFrame para o formato longo, necessário para o PandasDataset"""
    df = pd.read_csv(url, index_col=0, parse_dates=True)
    
    df_long = df.melt(
        id_vars=["dt"],
        var_name="item_id",
        value_name="value"
    )

    # Filtra apenas a coluna de interesse
    df_long = df_long[df_long["item_id"] == "Mercedes_level"]

    # Converte valores para float32
    df_long["value"] = pd.to_numeric(df_long["value"], errors="coerce").astype("float32")

    # Garantir que o timestamp está no tipo certo
    df_long["dt"] = pd.to_datetime(df_long["dt"], dayfirst=True)

    
    df_long = df_long.reset_index(drop=True)
    
    target_df = df_long[df_long["item_id"] == "Mercedes_level"].sort_values("dt")
    
    series = target_df["value"].values
    start_time = target_df["dt"].iloc[0]
    freq = pd.infer_freq(target_df["dt"]) or "1D"  # Inferir frequência

    return series, start_time, freq

# # Derreter o DataFrame para o formato longo
# df_long = df.melt(
#     id_vars=["dt"],
#     var_name="item_id",
#     value_name="value"
# )

# df_long = df_long[df_long["item_id"] == "Mercedes_level"]


# # Converter valores para float32
# df_long["value"] = pd.to_numeric(df_long["value"], errors="coerce").astype("float32")

# # Garantir que o timestamp está no tipo certo
# df_long["dt"] = pd.to_datetime(df_long["dt"])


# # Criar o dataset para previsão
# dataset = PandasDataset.from_long_dataframe(
#     df_long,
#     item_id="item_id",
#     target="value",
#     timestamp="dt"
# )


# df_long = df_long.reset_index(drop=True)
# target_df = df_long[df_long["item_id"] == "Mercedes_level"].sort_values("dt")

# series = target_df["value"].values
# start_time = target_df["dt"].iloc[0]
# freq = pd.infer_freq(target_df["dt"]) or "1D"  # Inferir frequência

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Use GPU if available, otherwise CPU

train_series, train_start_time, freq = convert_df_to_long(df_train)
val_series, _, _ = convert_df_to_long(df_val)
test_series, test_start_time, _ = convert_df_to_long(df)

# context_length = 32
# prediction_length = 7
stride = 1  

context_lengths = [32, 160, 352, 512]
horizon_lengths = [1, 7, 15, 30]
epochs = [2, 5, 10, 20]

# DataFrame base com as datas e valores observados
df_base = pd.DataFrame({"dt": df["dt"], "observed": df[target]})
df_base['dt'] = df_base['dt'].dt.floor('D')

for context_len in context_lengths:
    for horizon_len in horizon_lengths:
        train_dataset = create_sliding_window_dataset(
            series=train_series,
            start_time=train_start_time,
            freq=freq,
            context_length=context_len,
            prediction_length=horizon_len,
            stride=1
        )
        
        test_dataset = create_sliding_window_dataset(
            series=test_series,
            start_time=test_start_time,
            freq=freq,
            context_length=context_len,
            prediction_length=horizon_len,
            stride=1
        )
        for epoch in epochs:
            print(f"Context Length: {context_len}, Horizon Length: {horizon_len}, Epochs: {epoch}")
            
            # Previsão sem covariáveis
            simple_name = (f"e:{epoch}_Mercedes_{context_len}_{horizon_len}")
        #   simple_data = get_lag_llama_predictions(dataset=sliding_dataset, prediction_length=horizon_len, context_length=context_len, device=device, num_samples=1, forecast_name=simple_name)
            simple_data = finetune(
                context_length=context_len,
                prediction_length=horizon_len,
                num_samples=1,
                test=test_dataset,
                train=train_dataset,
                forecast_name=simple_name,
                epochs=epoch,
                device=device)
            # simple_data['dt_only'] = simple_data['dt'].dt.floor('D')

            df_base = df_base.merge(simple_data, on="dt", how="left")
            # print(df_base)

# Caminho para salvar no Google Drive (ajuste a pasta se quiser)
# df_base.drop(columns=['dt_only'], inplace=True)
output_path = "Mercedes_forecast_LagLlama_finetuned_multiple_epochs.csv"

# Salvar CSV
df_base.to_csv(output_path, index=False)






















# # @title forecast

# from matplotlib import pyplot as plt
# simple_data['forecast'].plot(kind='hist', bins=20, title='forecast')
# plt.gca().spines[['top', 'right',]].set_visible(False)

# # Derreter o DataFrame para o formato longo
# df_long = df.melt(
#     id_vars=["dt"],
#     var_name="item_id",
#     value_name="value"
# )

# df_long = df_long[df_long["item_id"] == "Mercedes_level"]

# # Converter valores para float32
# df_long["value"] = pd.to_numeric(df_long["value"], errors="coerce").astype("float32")

# # Garantir que o timestamp está no tipo certo
# df_long["dt"] = pd.to_datetime(df_long["dt"])


# # Criar o dataset para previsão
# dataset = PandasDataset.from_long_dataframe(
#     df_long,
#     item_id="item_id",
#     target="value",
#     timestamp="dt"
# )

# device = torch.device("cpu")


# df_long = df_long.reset_index(drop=True)
# target_df = df_long[df_long["item_id"] == "Mercedes_level"].sort_values("dt")

# series = target_df["value"].values
# start_time = target_df["dt"].iloc[0]
# freq = pd.infer_freq(target_df["dt"]) or "1D"  # Inferir frequência

# from google.colab import drive
# drive.mount('/content/drive')

# # hyperparams = [(50, 7, 1), (50, 7, 7), (50, 7, 30), (50, 160, 1), (50, 160, 7), (50, 160, 30), (50, 365, 1), (50, 365, 7), (50, 365, 30), (50, 500, 1), (50, 500, 7), (50, 500, 30)]
# hyperparams = [(30, 1)]

# # DataFrame base com as datas e valores observados
# df_base = pd.DataFrame({"dt": df["dt"], "observed": df[target]})


# for context_len, horizon_len in hyperparams:
#   sliding_dataset = create_sliding_window_dataset(
#     series=series,
#     start_time=start_time,
#     freq=freq,
#     context_length=context_len,
#     prediction_length=horizon_len,
#     stride=horizon_len
#   )

#   # Previsão sem covariáveis
#   simple_name = (f"Mercedes_{context_len}_{horizon_len}")
#   simple_data = get_lag_llama_predictions(dataset=backtest_dataset, prediction_length=horizon_len, context_length=context_len, device=device, num_samples=10, forecast_name=simple_name)

#   simple_data


#   df_base = df_base.merge(simple_data, on="dt", how="left")



# # Caminho para salvar no Google Drive (ajuste a pasta se quiser)
# output_path = "/content/drive/MyDrive/Resultados_Mercedes_forecast_LagLlama.csv"

# # Salvar CSV
# df_base.to_csv(output_path, index=False)

# print(f"CSV salvo em: {output_path}")

# """Plotar Resultados"""

# import pandas as pd
# import matplotlib.pyplot as plt

# from google.colab import drive
# drive.mount('/content/drive')

# # Caminho para o arquivo no seu Drive
# csv_path = "/content/drive/MyDrive/Resultados_Mercedes_forecast_LagLlama.csv"

# # Carregar com pandas
# df_result = pd.read_csv(csv_path)

# # Garante que a coluna de data seja interpretada como datetime
# df_result.iloc[:, 0] = pd.to_datetime(df_result.iloc[:, 0])

# # Nome das colunas
# dt_col = df_result.columns[0]
# observed_col = df_result.columns[1]
# model_cols = df_result.columns[2:]  # Todas as colunas dos modelos

# # Gera um gráfico para cada modelo comparado ao observado
# for model_col in model_cols:
#     plt.figure(figsize=(30, 8))
#     plt.plot(df_result[dt_col], df_result[observed_col], label="Observado", color='black', linewidth=2)
#     plt.plot(df_result[dt_col], df_result[model_col], label=f"Previsto ({model_col})", linestyle='--')

#     plt.xlabel("Data")
#     plt.ylabel("Valor")
#     plt.title(f"Comparação entre Observado e Previsto - Modelo: {model_col}")
#     plt.legend()
#     plt.grid(True)
#     plt.tight_layout()
#     plt.show()

# """Na previsão com o horizonte igual a um, percebe-se que com uma janela de contexto de 7 dias e usando todas as variáveis do dataframe, o modelo conseguiu obter o menor valor de MAE e com o contexto de 160 dias sem covariáveis obteu o menor MSE. Em geral, a previsão com estes dois conjuntos de hiperparametros obtiveram os melhores resultados entre as métricas.

# No horizonte com tamanho de 7 dias, os menores valores de MAE, MSE e RMSE foram produzidos por previsões com janelas de contexto maiores, como 300 e 500 dias, principalmente sem o uso de covariáveis, porém obtiveram péssimos valores de R-Squared, indicando que o modelo não consegue
# """

# import numpy as np
# import re

# # Colunas
# observed_col = df_result.columns[1]
# model_cols = df_result.columns[2:]

# # Lista para armazenar os resultados
# metric_results = []

# # Calcula as métricas para cada modelo
# for model_col in model_cols:
#     forecast = df_result[model_col]
#     target = df_result[observed_col]

#     # Remove pares com NaN
#     valid_idx = (~forecast.isna()) & (~target.isna())
#     forecast_clean = forecast[valid_idx].values
#     target_clean = target[valid_idx].values

#     # Calcula as métricas se houver dados válidos
#     if len(forecast_clean) > 0:
#         metrics = (
#             mae(forecast_clean, target_clean),
#             mse(forecast_clean, target_clean),
#             r_squared(forecast_clean, target_clean),
#             rmse(forecast_clean, target_clean),
#             pbias(forecast_clean, target_clean),
#             ve(forecast_clean, target_clean),
#         )
#     else:
#         metrics = (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)

#     metric_results.append((model_col, *metrics))

# # Criação do novo DataFrame
# metrics_df = pd.DataFrame(metric_results, columns=[
#     "Modelo", "MAE", "MSE", "R²", "RMSE", "PBIAS (%)", "VE"
# ])

# # Definir 'Tag' como índice
# metrics_df.set_index('Modelo', inplace=True)

# # Ajustar a regex para capturar o número no final da string (precedido ou não de traços, espaços etc.)
# metrics_df['numero_final'] = metrics_df.index.to_series().apply(
#     lambda x: int(re.search(r'(\d+)$', x).group(1)) if re.search(r'(\d+)$', x) else None
# )

# horizon_sizes = [1, 7, 30]

# # Gerar gráficos separados por tipo
# for horizon_size in horizon_sizes:
#     df_tipo = metrics_df[metrics_df['numero_final'] == horizon_size].drop(columns='numero_final')


#     for column in df_tipo.columns:
#         plt.figure(figsize=(10, 4))
#         plt.plot(df_tipo[column])
#         plt.title(f"Horizonte {horizon_size} - {column}")
#         plt.xlabel("Modelo")
#         plt.ylabel(column)
#         plt.grid(True)
#         plt.xticks(rotation=90)
#         plt.show()